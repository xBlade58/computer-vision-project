{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b7514a",
   "metadata": {},
   "source": [
    "# Computer Vision Project - Classification of Flowers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c11103",
   "metadata": {},
   "source": [
    "In this project your objective is to create a model in order to classify flowers. Thiszip file contains all relevant data. \n",
    "\n",
    "1. The data contains two folders: *train* and *test*. The *train* folder consists of 5486-images to use for training while the *test* folder contains 1351-images you can use to test your model in a **train-test-split** validation style. We have omitted another set of 1352 validation images which we will use to benchmark your final models in the last lecture. \n",
    "\n",
    "\n",
    "2. We have provided you with two label files: *train_labels.csv* and *test_labels.csv*. Each file contains the filename of the corresponding image and the class label. In total we have **102 different classes** of flowers.  You can import the label files using the `import_labels()` function provided to you in this notebook.\n",
    "\n",
    "\n",
    "3. Due to the large number of images, there is a good chance that you can not easily fit the entire training and testing data into RAM. We therefore give you an implementation of a `DataGenerator` class that can be used with keras. This class will read in the images from your hard-drive for each batch during during or testing. The class comes with some nice features that could improve your training significantly such as **image resizing**, **data augmentation** and **preprocessing**. Have a look at the code to find out how.\n",
    "\n",
    "    Initialize data generators using labels and image source directory.\n",
    "\n",
    "    `\n",
    "    datagen_train = DataGenerator('train', y_train, batch_size, input_shape, ...)\n",
    "    datagen_test = DataGenerator('test', y_test, batch_size, input_shape, ...)`\n",
    "\n",
    "    Train your model using data generators.\n",
    "\n",
    "    `model.fit(datagen_train, validation_data=datagen_test, ...)`\n",
    "    \n",
    "    \n",
    "4. Select a suitable model for classification. It is up to you to decide all model parameters, such as **number of layers**, **number and size of filter** in each layer, using **pooling** or, **image-size**, **data-augmentation**, **learning rate**, ... \n",
    "\n",
    "\n",
    "5. **Document** your progress and your intermediate results (your failures and improvements). Describe why you selected certain model and training parameters, what worked, what did not work. Store the training history (loss and accuracy) and create corresponding plots. This documentation will be part of your final presentation and will be **graded**.\n",
    "\n",
    "\n",
    "6. Feel free to explore the internet for suitable CNN models and re-use these ideas. If you use certain features we have not touched during the lecture such as Dropout, Residual Learning or Batch Normalization. Prepare a slide in your final presentation to explain in your own (basic) terms what these things to so we can all learn from your experience. **Notice:** Very large models might perform better but will be harder and slower to train. **Do not use a pre-trained model you find online!**\n",
    "\n",
    "\n",
    "7. Prepare a notebook with your model such that we can use it in the final competition. This means, store your trained model using `model.save(...)`. Your saved models can be loaded via `tf.keras.models.load_model(...)`. We will then provide you with a new folder containing images (*validation*) and a file containing labels (*validation_labels.csv*) which have the same structure. Prepare a data generator for this validation data (test it using the test data) and supply it to the \n",
    " `evaluate_model(model, datagen)` function provided to you.\n",
    " \n",
    " Your prepared notebook could look like this:\n",
    " \n",
    "    `... import stuff \n",
    "    ... code to load the stored model ...\n",
    "    y_validation = import_labels('validation_labels.csv')\n",
    "    datagen_validation = DataGenerator('validation', y_validation, batch_size, input_shape)\n",
    "    evaluate_model(model, datagen_validation)`\n",
    "\n",
    "\n",
    "8. Prepare a 15-Minute presentation of your findings and final model presentation. A rough guideline what could be interesting to your audience:\n",
    "    * Explain your models architecture (number of layers, number of total parameters, how long took it to train, ...)\n",
    "    * Compare the training history of your experimentats visually\n",
    "    * Explain your best model (why is it better)\n",
    "    * Why did you take certain decision (parameters, image size, batch size, ...)\n",
    "    * What worked, what did not work (any ideas why?)\n",
    "    * **What did you learn?**\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Input, BatchNormalization, GlobalAveragePooling2D, SeparableConv2D, Conv2DTranspose\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, img_root_dir, labels_dict, batch_size, target_dim, preprocess_func=None, use_augmentation=False):\n",
    "        self._labels_dict = labels_dict\n",
    "        self._img_root_dir = img_root_dir\n",
    "        self._batch_size = batch_size\n",
    "        self._target_dim = target_dim\n",
    "        self._preprocess_func = preprocess_func\n",
    "        self._n_classes = len(set(self._labels_dict.values()))\n",
    "        self._fnames_all = list(self._labels_dict.keys())\n",
    "        self._use_augmentation = use_augmentation\n",
    "\n",
    "        if self._use_augmentation:\n",
    "            self._augmentor = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rotation_range=40,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest'\n",
    "            )\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self._fnames_all)) / self._batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self._indices = np.arange(len(self._fnames_all))\n",
    "        np.random.shuffle(self._indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self._indices[index * self._batch_size:(index+1)*self._batch_size]\n",
    "\n",
    "        fnames = [self._fnames_all[k] for k in indices]\n",
    "        X,Y = self.__load_files__(fnames)\n",
    "\n",
    "        return X,Y\n",
    "\n",
    "    def __load_files__(self, batch_filenames):\n",
    "        X = np.empty((self._batch_size, *self._target_dim, 3))\n",
    "        Y = np.empty((self._batch_size), dtype=int)\n",
    "\n",
    "        for idx, fname in enumerate(batch_filenames):\n",
    "            img_path = os.path.join(self._img_root_dir, fname)\n",
    "            img = image.load_img(img_path, target_size=self._target_dim)\n",
    "            x = image.img_to_array(img)\n",
    "           \n",
    "            if self._preprocess_func is not None:\n",
    "                x = self._preprocess_func(x)\n",
    "\n",
    "            X[idx,:] = x \n",
    "            Y[idx] = self._labels_dict[fname]-1\n",
    "\n",
    "        if self._use_augmentation:\n",
    "            it = self._augmentor.flow(X, batch_size=self._batch_size, shuffle=False)\n",
    "            X = it.next()\n",
    "\n",
    "        if self._preprocess_func is not None:\n",
    "            X = self._preprocess_func(X)\n",
    "\n",
    "        return X, tf.keras.utils.to_categorical(Y, num_classes=self._n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30598081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in label file and return a dictionary {'filename' : label}.\n",
    "def import_labels(label_file):\n",
    "    labels = dict()\n",
    "\n",
    "    import csv\n",
    "    with open(label_file) as fd:\n",
    "        csvreader = csv.DictReader(fd)\n",
    "\n",
    "        for row in csvreader:\n",
    "            labels[row['filename']] = int(row['label'])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fa378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x):\n",
    "    return x / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_train = import_labels(\"train_labels.csv\")\n",
    "y_test = import_labels(\"test_labels.csv\")\n",
    "batch_size = 26\n",
    "input_shape = (224, 224)\n",
    "\n",
    "datagen_train = DataGenerator('train', y_train, batch_size, input_shape, preprocess_func=preprocessing, use_augmentation=True)\n",
    "datagen_test = DataGenerator('test', y_test, batch_size, input_shape, preprocess_func=preprocessing, use_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(test1[2,:] / 255)\n",
    "# plt.imshow(test1[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model():\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(Input(shape=(224,224,3)))\n",
    "\n",
    "    model.add(Conv2DTranspose(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(SeparableConv2D(128, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2DTranspose(512, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(SeparableConv2D(512, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(1024, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(Dense(102, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=['accuracy']) # try Nadam\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only provided you with the best model. \n",
    "# For the other models, please have a look at the documentation attached to the submission.\n",
    "model = best_model()\n",
    "history = model.fit(datagen_train, validation_data=datagen_test, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.grid(color='grey', linestyle='-', linewidth=0.2)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.yticks(np.arange(0, 1, step=0.05))\n",
    "plt.xticks(np.arange(0, 41, step=2))\n",
    "plt.grid()\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from collections import defaultdict\n",
    "color_map = defaultdict(dict)\n",
    "color_map[layers.Conv2D]['fill'] = '#00f5d4'\n",
    "color_map[layers.MaxPooling2D]['fill'] = '#993333'\n",
    "color_map[layers.Conv2DTranspose]['fill'] = '#ffff00'\n",
    "color_map[layers.Dense]['fill'] = '#ff0000'\n",
    "color_map[layers.SeparableConv2D]['fill'] = '#abce00'\n",
    "color_map[layers.BatchNormalization]['fill'] = '#0000ff'\n",
    "color_map[layers.GlobalAveragePooling2D]['fill'] = '#ff33cc'\n",
    "visualkeras.layered_view(model, legend=True, font=font,color_map=color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate with data provided by the lectureres\n",
    "\n",
    "validation_model = tf.keras.models.load_model(\"models/model7_40-60ep\")\n",
    "y_validation = import_labels('validation_labels.csv')\n",
    "datagen_validation = DataGenerator('validation', y_validation, batch_size, input_shape)\n",
    "score = validation_model.evaluate(datagen_validation, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb69c4671c883ed6a4df5072a4d94acbf114d31e22cae574724676294076912d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
